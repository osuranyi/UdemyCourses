{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Section3_BayesClassifiers.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPshnHb9drLraPm0Cxpo8tK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Section 3: Naive Bayes and Bayes classifiers** "],"metadata":{"id":"j6YGQq4FoF6E"}},{"cell_type":"markdown","source":["Bayes rule:\n","\n","\\begin{align}\n","p(y|x) = \\frac{p(x|y)p(y)}{p(x)}\n","\\end{align}\n","\n","The name of the terms in the above expression are:\n","\n","\\begin{align}\n","p(y) \\, &\\Rightarrow \\, \\text{prior probability} \\\\\n","p(y|x) \\, &\\Rightarrow \\, \\text{posterior probability} \\\\\n","p(x|y) \\, &\\Rightarrow \\, \\text{likelihood} \\\\\n","p(x) \\, &\\Rightarrow \\, \\text{evidence}\n","\\end{align}"],"metadata":{"id":"-sJMhA3TotOK"}},{"cell_type":"markdown","source":["The output of a Bayes classifier is\n","\n","\\begin{align}\n","\\underset{c}{\\mathrm{argmax}} \\, p(y=c|x).\n","\\end{align}\n","\n","We can see that the result does not depend on $p(x)$ in the denominator; therefore, it can be discarded in the calculation. Furthermore, we might use the logarithm of the above expression, which would simplify the formulas when the likelihood is Gaussian:\n","\n","\\begin{align}\n","\\underset{c}{\\mathrm{argmax}} \\, \\log p(x|y=c) + \\log p(y=c)\n","\\end{align}"],"metadata":{"id":"qVqK4eZr85i6"}},{"cell_type":"markdown","source":["## **Naive Bayes**"],"metadata":{"id":"8mklp4M3wcvk"}},{"cell_type":"markdown","source":["When we have $D$ features, denoted as $x_1$, $x_2$, $\\dots$, $x_D$:\n","\n","\\begin{align}\n","p(y|x_1,x_2,\\dots,x_D) \\propto p(x_1,x_2,\\dots,x_D|y) p(y).\n","\\end{align}\n","\n","In a Naive Bayes classifier, we assume that the $x_i$ variables are independent:\n","\n","\\begin{align}\n","p(y|x_1,x_2,\\dots,x_D) \\propto p_1(x_1|y)p_2(x_2|y) \\dots p_D(x_D|y) p(y).\n","\\end{align}\n","\n","Thus, the output of the classifier will be\n","\n","\\begin{align}\n","\\underset{c}{\\mathrm{argmax}} \\left[ \\sum_{i=1}^D \\log p_i(x_i|y=c) + \\log p(y=c) \\right]\n","\\end{align}\n","\n","When p_i(x|y) is Gaussian, we get:\n","\n","\\begin{align}\n","\\underset{c}{\\mathrm{argmax}} \\left[-\\frac{1}{2}\\sum_{i=1}^D \\log\\left( 2\\pi\\sigma_{i|c}^2\\right)  -\\sum_{i=1}^D \\frac{(x_i-\\mu_{i|c})^2}{2\\sigma_{i|c}^2} + \\log p(y=c) \\right],\n","\\end{align}\n","\n","We might extract the constant $2\\pi$ from the first term and discard it, as it will have the same value for any $c$, thus simplifying the above expression as:\n","\n","\\begin{align}\n","\\underset{c}{\\mathrm{argmax}} \\left[-\\frac{1}{2}\\sum_{i=1}^D \\log\\sigma_{i|c}^2  -\\sum_{i=1}^D \\frac{(x_i-\\mu_{i|c})^2}{2\\sigma_{i|c}^2} + \\log p(y=c) \\right].\n","\\end{align}"],"metadata":{"id":"RCBUlVBgwjQz"}},{"cell_type":"markdown","source":["### *''Handwritten'' example*\n","\n","* Spam classifier\n","* 3 inputs: \"money\", \"free\", \"pills\" (1, if occurs in email, 0 otherwise)\n","\n","| Money | Free | Pills | Spam? |\n","|:-----:|:----:|:-----:|:-----:|\n","|   1   |   1  |   1   |   1   |\n","|   0   |   1  |   1   |   1   |\n","|   1   |   0  |   1   |   1   |\n","|   0   |   1  |   1   |   1   |\n","|   1   |   1  |   0   |   1   |\n","|   1   |   0  |   0   |   0   |\n","|   0   |   0  |   0   |   0   |\n","|   0   |   1  |   0   |   0   |\n","|   1   |   0  |   0   |   0   |\n","|   0   |   0  |   0   |   0   |\n","\n","So, using this table, we estimate the probabilities as:\n","\\begin{align}\n","p(y=1) &= \\frac{1}{2}, \\quad p(y=0) = \\frac{1}{2}, \\\\\n","p(\\text{money}|y=1) &= \\frac{3}{5}, \\quad p(\\text{money}|y=0) = \\frac{2}{5}, \\\\\n","p(\\text{free}|y=1) &= \\frac{4}{5}, \\quad p(\\text{free}|y=0) = \\frac{1}{5}, \\\\\n","p(\\text{pills}|y=1) &= \\frac{4}{5}, \\quad p(\\text{pills}|y=0) = 0 \\\\\n","\\end{align}\n","\n","The calculation for the first row is the following:\n","\\begin{align}\n","p(y=1|x_i) \\propto p(\\text{money}|y=1) \\cdot p(\\text{free}|y=1) \\cdot p(\\text{pills}|y=1) \\cdot p(y=1) = \\frac{3}{5} \\cdot \\frac{4}{5} \\cdot \\frac{4}{5} \\cdot \\frac{1}{2} = \\frac{24}{125} = 0.192 \\\\\n","p(y=0|x_i) \\propto p(\\text{money}|y=0) \\cdot p(\\text{free}|y=0) \\cdot p(\\text{pills}|y=0) \\cdot p(y=0) = \\frac{2}{5} \\cdot \\frac{1}{5} \\cdot 0 \\cdot \\frac{1}{2} = 0 \\\\\n","\\end{align}"],"metadata":{"id":"vtUHawPxFrKx"}},{"cell_type":"markdown","source":["The first email is classified as spam, since $p(y=1|x_i) > p(y=0|x_i)$.\n","We can also notice, that all of the emails, which contains the word \"pills\" will be always classified as spam, because in this dataset, there are no such non-spam email which contains this word, therefore $p(y=0|\\text{pills})$ is always zero. So the first four rows in the table are classified as spam. Let's calculate the rest:\n","\n","* 5th row - classified as spam:\n","\\begin{align}\n","p(y=1|x_i) &\\propto \\frac{3}{5} \\cdot \\frac{4}{5} \\cdot \\left(1-\\frac{4}{5}\\right) \\cdot \\frac{1}{2} = \\frac{6}{125} = 0.048 \\\\\n","p(y=0|x_i) &\\propto \\frac{2}{5} \\cdot \\frac{1}{5} \\cdot (1-0) \\cdot \\frac{1}{2} = \\frac{1}{25} = 0.04\n","\\end{align}\n","\n","* 6th row - classified as non-spam:\n","\\begin{align}\n","p(y=1|x_i) &\\propto \\frac{3}{5} \\cdot \\left(1-\\frac{4}{5}\\right) \\cdot \\left(1-\\frac{4}{5}\\right) \\cdot \\frac{1}{2} = \\frac{3}{250} = 0.06 \\\\\n","p(y=0|x_i) &\\propto \\frac{2}{5} \\cdot \\left(1-\\frac{1}{5}\\right) \\cdot (1-0) \\cdot \\frac{1}{2} = \\frac{4}{25} = 0.16\n","\\end{align}\n","\n","* 7th row - classified as non-spam:\n","\\begin{align}\n","p(y=1|x_i) &\\propto \\left(1-\\frac{3}{5}\\right) \\cdot \\left(1-\\frac{4}{5}\\right) \\cdot \\left(1-\\frac{4}{5}\\right) \\cdot \\frac{1}{2} = \\frac{1}{125} = 0.008 \\\\\n","p(y=0|x_i) &\\propto \\left(1-\\frac{2}{5}\\right) \\cdot \\left(1-\\frac{1}{5}\\right) \\cdot (1-0) \\cdot \\frac{1}{2} = \\frac{6}{25} = 0.24\n","\\end{align}\n","\n","* 8th row - classified as non-spam:\n","\\begin{align}\n","p(y=1|x_i) &\\propto \\left(1-\\frac{3}{5}\\right) \\cdot \\frac{4}{5} \\cdot \\left(1-\\frac{4}{5}\\right) \\cdot \\frac{1}{2} = \\frac{4}{125} = 0.032 \\\\\n","p(y=0|x_i) &\\propto \\left(1-\\frac{2}{5}\\right) \\cdot \\frac{1}{5} \\cdot (1-0) \\cdot \\frac{1}{2} = \\frac{3}{50} = 0.06\n","\\end{align}\n","\n","* 9th row - classified as non-spam:\n","\\begin{align}\n","p(y=1|x_i) &\\propto \\frac{3}{5} \\cdot \\left(1-\\frac{4}{5}\\right) \\cdot \\left(1-\\frac{4}{5}\\right) \\cdot \\frac{1}{2} = \\frac{3}{250} = 0.012 \\\\\n","p(y=0|x_i) &\\propto \\frac{2}{5} \\cdot \\left(1-\\frac{1}{5}\\right) \\cdot (1-0) \\cdot \\frac{1}{2} = \\frac{4}{25} = 0.16\n","\\end{align}\n","\n","* 10th row - classified as non-spam: same as 7th row.\n","\n","The results are summarized in the last column of this table:\n","\n","| Money | Free | Pills | Spam? | Bayes output |\n","|:-----:|:----:|:-----:|:-----:|--------------|\n","|   1   |   1  |   1   |   1   | 1            |\n","|   0   |   1  |   1   |   1   | 1            |\n","|   1   |   0  |   1   |   1   | 1            |\n","|   0   |   1  |   1   |   1   | 1            |\n","|   1   |   1  |   0   |   1   | 1            |\n","|   1   |   0  |   0   |   0   | 0            |\n","|   0   |   0  |   0   |   0   | 0            |\n","|   0   |   1  |   0   |   0   | 0            |\n","|   1   |   0  |   0   |   0   | 0            |\n","|   0   |   0  |   0   |   0   | 0            |"],"metadata":{"id":"cf3dHvRnLa_U"}},{"cell_type":"markdown","source":["### *Laplace smoothing*\n","\n","We saw, that when we have a 0 probability, we end up with the whole thing equal zero. A possible solution is Laplace smoothing:\n","\\begin{align}\n","p(x_i|c) = \\frac{\\mathrm{count}(x_i|y=c) + 1}{\\mathrm{count}(y=c)+V},\n","\\end{align}\n","where $V$ is the vocabulary size, which ensures the correct normalization. Using Laplace smoothing, we get the following probabilities:\n","\\begin{align}\n","p(y=1) &= \\frac{1}{2}, \\quad p(y=0) = \\frac{1}{2}, \\\\\n","p(\\text{money}|y=1) &= \\frac{1}{2}, \\quad p(\\text{money}|y=0) = \\frac{3}{8}, \\\\\n","p(\\text{free}|y=1) &= \\frac{5}{8}, \\quad p(\\text{free}|y=0) = \\frac{1}{4}, \\\\\n","p(\\text{pills}|y=1) &= \\frac{5}{8}, \\quad p(\\text{pills}|y=0) = \\frac{1}{8}.\n","\\end{align}\n","Using these, we can carry out the Naive Bayes calculations in a similar fashion like before.\n"],"metadata":{"id":"xE6wlwwnzlQg"}},{"cell_type":"markdown","source":["## *Naive Bayes in code*"],"metadata":{"id":"fpfUI-0zhiQF"}},{"cell_type":"code","source":["# Loading MNIST data\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from keras.datasets import mnist\n","\n","(train_X, train_y), (test_X, test_y) = mnist.load_data()\n","\n","# converting 2D images to 1D vectors\n","train_X = train_X.reshape(len(train_X),28*28)\n","test_X = test_X.reshape(len(test_X),28*28)\n","\n","# normalizing features\n","train_X = train_X / 255\n","test_X  = test_X / 255"],"metadata":{"id":"CqYuWsvZhm6B","executionInfo":{"status":"ok","timestamp":1643901925312,"user_tz":300,"elapsed":5626,"user":{"displayName":"Olivér Surányi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKl0SxJ_LX9cd5O64Ost2_QTOMXzeDfLJjtch6ig=s64","userId":"15261409344413336958"}},"outputId":"0a505d37-bc73-4efe-ee00-ecfe8047bb15","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["# Naive Bayes classifier class with continuous, Gaussian features\n","\n","class NaiveBayesClassifier:\n","  def fit(self,X,y,smoothing=1E-6):\n","    N,D = X.shape[0], X.shape[1]\n","    category_counts = np.unique(y,return_counts=True)\n","    self.categories = category_counts[0]\n","    counts = category_counts[1]\n","    C = len(self.categories)\n","\n","    # initializing mean and variance matrices\n","    self.mean = np.zeros((C,D))\n","    self.variance = np.zeros((C,D))\n","\n","    # calculating mean and variances for each categories\n","    for i,c in enumerate(self.categories):\n","      self.mean[i,:] = X[y == c].sum(axis=0) / N\n","      self.variance[i,:] = ((X[y == c] - self.mean[i,:])**2).sum(axis=0) / (N - 1) + smoothing\n","      # More compact form, but interestingly found to be less accurate,\n","      # var() method might calculated the biased variance? (dividing by N not N-1)\n","      #self.mean[i,:] = X[y == c].mean(axis=0)\n","      #self.variance[i,:] = X[y == c].var(axis=0) + smoothing\n","\n","    # calculating p(Y=c) probabilities\n","    self.log_pY = np.log(counts / sum(counts))\n","\n","    # calculating the sum of logarithms of variances\n","    # setting zero variances to 1 in order to avoid 0 in logarithms and division by zero\n","    #self.variance[self.variance < 1E-10] = 1\n","    self.sum_log_var = -0.5*np.log(self.variance).sum(axis=1)\n","\n","  def predict(self,X):\n","    N,D = X.shape[0], X.shape[1]\n","\n","    predictions = np.zeros(N,dtype=int)\n","\n","    for i in range(N):\n","      max_index = np.argmax(0.5*( (self.mean - X[i,:]) / self.variance ).sum(axis=1) + self.log_pY + self.sum_log_var)\n","      predictions[i] = self.categories[max_index]\n","\n","    return predictions\n","\n","  def score(self,X,y):\n","    return np.mean(y == self.predict(X))"],"metadata":{"id":"mdQsz3yAjsh-","executionInfo":{"status":"ok","timestamp":1643904938512,"user_tz":300,"elapsed":139,"user":{"displayName":"Olivér Surányi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKl0SxJ_LX9cd5O64Ost2_QTOMXzeDfLJjtch6ig=s64","userId":"15261409344413336958"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["model = NaiveBayesClassifier()\n","model.fit(train_X,train_y)\n","print(\"Train accuracy\",model.score(train_X,train_y))\n","print(\"Test accuracy\",model.score(test_X,test_y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cH1QQt9qTN3X","executionInfo":{"status":"ok","timestamp":1643905113109,"user_tz":300,"elapsed":2757,"user":{"displayName":"Olivér Surányi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKl0SxJ_LX9cd5O64Ost2_QTOMXzeDfLJjtch6ig=s64","userId":"15261409344413336958"}},"outputId":"1b9af500-7398-4b25-b65b-2553e02db1fb"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Train accuracy 0.7584666666666666\n","Test accuracy 0.764\n"]}]},{"cell_type":"markdown","source":["## **Non-naive Bayes classifier**"],"metadata":{"id":"eaZbZd2VzkyM"}},{"cell_type":"markdown","source":["Independence is not assumed, thus $p(x_1,x_2,\\dots,x_D|c)$ does not factorizes.\n","How do we model $p(X|C)$?\n","* Full covariance\n","* Hidden Markov model\n","* Custom Bayes Net\n","* Etc.\n","\n","In this case, we will use multivariate Gaussian with full covariances calculated."],"metadata":{"id":"DjfEzPtY0vn0"}},{"cell_type":"code","source":["# This time, using SciPy's multivariate normal distribution\n","from scipy.stats import multivariate_normal as mvn"],"metadata":{"id":"C64pNX6X1Zlc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Bayes classifier class with continuous, Gaussian features\n","\n","class BayesClassifier:\n","  def fit(self,X,y,smoothing=1E-2):\n","    N,D = X.shape[0], X.shape[1]\n","    category_counts = np.unique(y,return_counts=True)\n","    self.categories = category_counts[0]\n","    counts = category_counts[1]\n","    C = len(self.categories)\n","\n","    # initializing mean and variance matrices\n","    self.mean = np.zeros((C,D))\n","    self.cov = np.zeros((C,D,D))\n","\n","    # calculating mean and variances for each categories\n","    for i,c in enumerate(self.categories):\n","      self.mean[i,:] = X[y == c].mean(axis=0)\n","      self.cov[i,:] = np.cov(X[y == c],rowvar=False) + np.eye(D)*smoothing\n","\n","    # calculating p(Y=c) probabilities\n","    self.log_pY = np.log(counts / sum(counts))\n","\n","  def predict(self,X):\n","    N,D = X.shape[0], X.shape[1]\n","    C = len(self.categories)\n","\n","    outputs = np.zeros((N,C))\n","    predictions = np.zeros(N,dtype=int)\n","\n","    for i in range(C):\n","      outputs[:,i] = mvn.logpdf(X,self.mean[i],self.cov[i]) + self.log_pY[i]\n","      \n","    max_index = np.argmax(outputs,axis=1)\n","    predictions = self.categories[max_index]\n","\n","    return predictions\n","\n","  def score(self,X,y):\n","    return np.mean(y == self.predict(X))"],"metadata":{"id":"ssVqlFXo1zPc","executionInfo":{"status":"ok","timestamp":1643904964553,"user_tz":300,"elapsed":236,"user":{"displayName":"Olivér Surányi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKl0SxJ_LX9cd5O64Ost2_QTOMXzeDfLJjtch6ig=s64","userId":"15261409344413336958"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["model2 = BayesClassifier()\n","model2.fit(train_X,train_y)\n","print(\"Train accuracy\",model2.score(train_X,train_y))\n","print(\"Test accuracy\",model2.score(test_X,test_y))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNdzW8oW22d0","executionInfo":{"status":"ok","timestamp":1643905178331,"user_tz":300,"elapsed":38250,"user":{"displayName":"Olivér Surányi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgKl0SxJ_LX9cd5O64Ost2_QTOMXzeDfLJjtch6ig=s64","userId":"15261409344413336958"}},"outputId":"4851f410-8ee6-4d07-9f66-86c85c671e90"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Train accuracy 0.9555333333333333\n","Test accuracy 0.9473\n"]}]}]}